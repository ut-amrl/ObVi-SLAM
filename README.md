# ObVi-SLAM
ObVi-SLAM is a joint object-visual SLAM approach aimed at long-term multi-session robot deployments. 

[[Paper](https://arxiv.org/abs/2309.15268)] [[Video](https://youtu.be/quJOgnEdaZ0)]

Offline execution instructions coming soon. 
ROS implementation coming late 2023/early 2024. 

Please email amanda.adkins4242@gmail.com with any questions! 


## Evaluation
For information on how to set up and run the comparison algorithms, see our [evaluation repo](https://github.com/ut-amrl/ObVi-SLAM-Evaluation).


## Extended Results
See the [version of our paper](https://drive.google.com/file/d/1Cf6QfheKa09mJO8oqgUqdTUC3y12JXRN/view?usp=share_link) with an appendix containing extended results and the full ablation study details.


## Installation Instructions
TODO
- dockerfile version (recommended)
- native version

## Minimal Execution Instructions
TODO
- Explain files needed and their structure (intrinsics, extrinsics, visual features, bounding box (opt), images?,
- Explain how to run given these files


## Results from ROS bag
TODO
- Explain how to preprocess rosbag to get the data needed for minimal execution above

## Configuration File Guide
TODO 
- Explain how to modify configuration file -- which parameters will someone need to modify for different environment, (lower priority): explain each of the parameters in the config file

## Evaluation
Our YOLO model: TODO

## TODOs
- Add installation instructions
- Add offline execution instructions
- Add YOLO model

